{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial 00\n",
    "mb = pd.read_csv(\"Data/microbiome.csv\", sep=',')\n",
    "# Homework 1\n",
    "df = pd.read_csv(\"data/data.tsv.gz\", delimiter = r'\\t', compression = 'gzip', engine = 'python', on_bad_lines= 'skip')\n",
    "# Homework 2\n",
    "df_raw = pd.read_csv('data/pitchfork.csv.gz', compression='gzip', engine='python')\n",
    "# Tutorial 03\n",
    "commune_pop = pd.read_excel(data_folder+'communes_pop.xls',skiprows=5,skipfooter=7,\n",
    "                            sheet_name='2017',names=['commune','population_Jan',\n",
    "                                                     'birth_alive','death','natrual_increase',\n",
    "                                                     'arrivals','departure','migratory_balance',\n",
    "                                                     'divergence_statistic','population_Dec','variation_num',\n",
    "                                                     'variation_ratio'])\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates(subset=['id'])\n",
    "\n",
    " # Good practice when opening files\n",
    "with open('file', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply function to each element in series: Tutorial 07 - Applied ML before doing logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['adopted'] = data.outcome_type.apply(lambda r: 1 if r=='Adoption' else 0) # apply function to each element r of data.outcome_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns / rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = seeds_.drop(columns=['ID', 'seedType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_CI(data, nbr_draws):\n",
    "    \"\"\" \n",
    "    This is a helper bootstrapping function you can use in this exercise to obtain 95% confidence\n",
    "    intervals around the estimated average. The underlying concept will be explained in the next week's lecture.\n",
    "\n",
    "    Input: your array and the number of random samples (e.g., 1000 is a good number)\n",
    "    Output: [lower error, upper error] \n",
    "    \"\"\"\n",
    "    means = np.zeros(nbr_draws)\n",
    "    data = np.array(data)\n",
    "\n",
    "    for n in range(nbr_draws):\n",
    "        indices = np.random.randint(0, len(data), len(data))\n",
    "        data_tmp = data[indices] \n",
    "        means[n] = np.nanmean(data_tmp)\n",
    "\n",
    "    return [np.nanpercentile(means, 2.5),np.nanpercentile(means, 97.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_column(column):\n",
    "    \"\"\" \n",
    "    Input: pandas dataframe column, or series\n",
    "    Output: column with mean 0 and std deviation 1, minimum value -1 maximum value +1\n",
    "    Obs: not a good way to standardize the column if the column's distribution is a power law\n",
    "    \"\"\" \n",
    "    return (column - column.mean()) / column.std()\n",
    "\n",
    "def min_max_normalize_column(column):\n",
    "    \"\"\"\n",
    "    Input: pandas dataframe column, or series\n",
    "    Output: column with min value 0 and max value 1\n",
    "    Obs: not a good way to standardize the column if the column's distribution is a power law\n",
    "    \"\"\"\n",
    "    return (column - column.min()) / (column.max() - column.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN in any column\n",
    "print(original_data.shape)\n",
    "data = original_data.dropna(axis=0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare continous+categorical data for logistic regression: see Tutorial 07 - Applied ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "\n",
    "plt.rc('axes', labelsize=14)   # fontsize of the x and y labels\n",
    "plt.rc('axes', titlesize=14)\n",
    "plt.rc('xtick', labelsize=13)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=13)\n",
    "plt.rc('legend', fontsize=14)    # legend fontsize\n",
    "plt.rc('figure', titlesize=14)   # fontsize of the figure title\n",
    "plt.rc('lines', markersize=7)\n",
    "plt.rc('lines', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe column (gives mean, median, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball.player.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram (1D plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "baseball['one_specific_column'].hist(bins=nr_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barplot (1D plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count = article_df.topic.value_counts()\n",
    "plt.bar(topic_count.index, topic_count.values)\n",
    "plt.xticks(topic_count.index, rotation=80)\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Topic count')\n",
    "plt.title('Number of articles per topic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"barplot-vertical\n",
    ".png\" style=\"width: 400px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar; increased readability\n",
    "y_pos = np.arange(len(topic_count.index))\n",
    "plt.barh(y_pos, topic_count.values, align='center')\n",
    "plt.yticks(y_pos, labels=topic_count.index)\n",
    "plt.xlabel('Topic count')\n",
    "plt.title('Number of articles per topic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"barplot-h.png\" style=\"width: 400px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot (1D plot): the lower line is the lower quartile of the data (25% of the data), the middle line is the mean, the upper line is the upper quartile (75% of the data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boxplot.png\" style=\"width: 400px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot (2D plot): Tutorial 05 - Observational studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.boxplot(x=\"treat\", y=\"age\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boxplot-2d.png\" style=\"width: 400px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countplot (2D plot): Tutorial 05 - Observational studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.countplot(x=\"treat\", hue=\"married\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"countplot.png\" style=\"width: 400px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter (2D plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "dataframe.plot.scatter(x='column_on_x_axis', y='column_on_y_axis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line (2D plot) with confidence interval: Tutorial 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial 02 Becoming a DataVizard\n",
    "# Write your code to make plot with errorbars here\n",
    "# plt.scatter(years, avg_revenue, s=15)\n",
    "confidence_interval = np.zeros((2, len(years)))\n",
    "for i in range(len(years)):\n",
    "    data = movies.loc[movies.year == years[i]].worldwide_gross\n",
    "    confidence_interval[:, i] = np.array(bootstrap_CI(data, 1000))\n",
    "\n",
    "# Write your code to make filled plot here\n",
    "plt.fill_between(years, confidence_interval[0,:], confidence_interval[1,:], color='gray')\n",
    "plt.errorbar(years, avg_revenue, yerr=None, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"output.png\" style=\"width: 500px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('xlabel')\n",
    "plt.ylabel('ylabel')\n",
    "plt.title('title')\n",
    "plt.xticks(['list of values for x-axis']) # usually used in barplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(8,6), sharey=True)\n",
    "# axs[0, 0].hist(...)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting probability to log odds (Tutorial 04 - Regression analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def p_to_log_odds(p):\n",
    "    return np.log(p/(1-p))\n",
    "def log_odds_to_p(odds):\n",
    "    return np.exp(odds) / (1+ np.exp(odds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, auc, roc_curve\n",
    "\n",
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")\n",
    "\n",
    "# Precision: avoid false positives\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (precision.mean(), precision.std() * 2))\n",
    "# Recall: avoid false negatives\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (recall.mean(), recall.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_metrics(true_values, preds, print_confusion_matrix=False):\n",
    "    tn, fp, fn, tp = confusion_matrix(true_values, preds).ravel()\n",
    "    if print_confusion_matrix:\n",
    "        print(f'Confusion matrix:\\n  TP: {tp} | FN: {fn} \\n  FP: {fp} | TN: {tn} ')\n",
    "    accuracy  = (tn + tp) / (tn + fp + fn + tp)\n",
    "\n",
    "    # Wrt positive classes\n",
    "    precision_pos = tp / (tp + fp)    # what fraction of those classified as positive is actually positive?\n",
    "    recall_pos    = tp / (tp + fn)    # (True Positive Rate) of all true positives, how many did I correctly classify as positive?\n",
    "    f1_score_pos  = 2 * precision_pos * recall_pos / (precision_pos + recall_pos)\n",
    "\n",
    "    # Wrt negative classes\n",
    "    precision_neg = tn / (tn + fn)\n",
    "    recall_neg    = tn / (tn + fp)    # (True Negative Rate) of all true negatives, how many did I correctly classify as negative\n",
    "    f1_score_neg  = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "    return [accuracy, precision_pos, recall_pos, f1_score_pos, precision_neg, recall_neg, f1_score_neg]\n",
    "\n",
    "[accuracy, precision_pos, recall_pos, f1_score_pos, precision_neg, recall_neg, f1_score_neg] = get_metrics(Y_test, preds, True)\n",
    "\n",
    "print(f'Metrics:\\n - accuracy = {accuracy:.3f}\\n POSITIVE CLASS\\n - precision = {precision_pos:.3f}\\n - recall = {recall_pos:.3f}\\n - f1_score = {f1_score_pos:.3f} \\n NEGATIVE CLASS\\n - precision = {precision_neg:.3f}\\n - recall = {recall_neg:.3f}\\n - f1_score = {f1_score_neg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cf_matrix = confusion_matrix(y_test, classifier.predict(X_test))\n",
    "sns.heatmap(cf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking linear correlation between two variables (Pearson number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial 03 - Describing Data\n",
    "stats.pearsonr(df['IncomePerCap'], df['Employed'])\n",
    "# Result: (0.2646136320394488, 9.942215354240067e-53)\n",
    "# Interpretation: There is a small (0.26), but significant (p = 9e-53 < 0.05) positive linear correlation betweem IncomePerCap and Employed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode (exam 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_b5 = pd.get_dummies(youtube_ml, columns=['channel_cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split (exam 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Split --- FASTER!\n",
    "def split(data_to_split, ratio):\n",
    "    mask = np.random.rand(len(data_to_split)) < ratio\n",
    "    return [data_to_split.loc[mask], data_to_split[~mask]]\n",
    "[train, test] = split(data, 0.8)\n",
    "\n",
    "# or with sklearn:\n",
    "# TODO\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_columns = youtube_ml_ohe.columns.tolist()\n",
    "X_columns.remove('view_count')\n",
    "Y = youtube_ml_ohe['view_count']\n",
    "X = youtube_ml_ohe.loc[:, X_columns]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Another example with sklearn\n",
    "train, test = train_test_split(youtube_ml, test_size=0.3, random_state=42)\n",
    "X_train = train.drop(columns=['view_count', 'channel'])\n",
    "Y_train = train.view_count.apply(lambda nr_views : nr_views > train.view_count.median())\n",
    "X_test = test.drop(columns=['view_count', 'channel'])\n",
    "Y_test = test.view_count.apply(lambda nr_views : nr_views > test.view_count.median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression (exam 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logreg_hyper = {'C': (1, 10, 100)}\n",
    "logreg_cv = GridSearchCV(logreg, param_grid=logreg_hyper, cv=3)\n",
    "logreg_cv.fit(X_train, Y_train_logreg)\n",
    "print(f'The accuracy with the channels is {100*accuracy_score(Y_test_logreg, logreg_cv.predict(X_test)):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression SKLEARN with hyper-parameter grid search (exam 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge = linear_model.Ridge()\n",
    "ridge_hyper = {'alpha': (0.001, 0.01, 0.1)}\n",
    "ridge_cv = GridSearchCV(ridge, ridge_hyper, cv=3)\n",
    "ridge_cv.fit(X_train, Y_train)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(Y_test, ridge_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression SEABORN with confidence levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial 03 - Describing data\n",
    "import seaborn as sns\n",
    "sns.lmplot(x='SelfEmployed', y='IncomePerCap', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lmplot.png\" style=\"width: 500px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression STATSMODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "# Declare model\n",
    "mod = smf.logit(formula='DEATH_EVENT ~  age + creatinine_phosphokinase + ejection_fraction + \\\n",
    "                        platelets + serum_creatinine + serum_sodium + \\\n",
    "                        C(diabetes) + C(high_blood_pressure) +\\\n",
    "                        C(sex) + C(anaemia) + C(smoking) + C(high_blood_pressure)', data=df)\n",
    "# Fit optimal parameters\n",
    "res = mod.fit()\n",
    "print(res.summary())\n",
    "# feature names\n",
    "variables = res.params.index\n",
    "# quantifying uncertainty!\n",
    "# coefficients\n",
    "coefficients = res.params.values\n",
    "# p-values\n",
    "p_values = res.pvalues\n",
    "# standard errors\n",
    "standard_errors = res.bse.values\n",
    "#confidence intervals\n",
    "res.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression STATSMODELS for finding propensity score: Tutorial 05 - Observational studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's standardize the continuous features\n",
    "lalonde_data['age'] = (lalonde_data['age'] - lalonde_data['age'].mean())/lalonde_data['age'].std()\n",
    "lalonde_data['educ'] = (lalonde_data['educ'] - lalonde_data['educ'].mean())/lalonde_data['educ'].std()\n",
    "lalonde_data['re74'] = (lalonde_data['re74'] - lalonde_data['re74'].mean())/lalonde_data['re74'].std()\n",
    "lalonde_data['re75'] = (lalonde_data['re75'] - lalonde_data['re75'].mean())/lalonde_data['re75'].std()\n",
    "\n",
    "mod = smf.logit(formula='treat ~  age + educ + C(black) + C(hispan)  + C(married) + C(nodegree) + \\\n",
    "        +re74 + re75', data=lalonde_data)\n",
    "\n",
    "res = mod.fit()\n",
    "\n",
    "# Extract the estimated propensity scores\n",
    "lalonde_data['Propensity_score'] = res.predict()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans: Tutorial 08 - Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def run_kmeans(features, start, end):\n",
    "    \"\"\"\n",
    "    Runs the k-prototypes clustering method for several values of k.\n",
    "    Returns: a dictionary containing the result where the key is k and the value is\n",
    "             the result of clustering with k, e.g. kprotos = {2: kproto_2, 3: kproto_3}.\n",
    "    \"\"\"\n",
    "    kprotos = {}\n",
    "    for k in range(start, end + 1):\n",
    "        # Assign the labels to the clusters\n",
    "        # kproto = KPrototypes(n_clusters=k, random_state=10, n_jobs=-1).fit(features, categorical=[1, 2])\n",
    "        kproto = KMeans(n_clusters=k, random_state=42).fit(features)\n",
    "        kprotos[k] = kproto\n",
    "    return kprotos\n",
    "\n",
    "def plot_sse(kprotos):\n",
    "    # Plot sum of squared errors (SSE): use elbow method for best k\n",
    "    # plt.plot(kprotos.keys(), [kproto.cost_ for _, kproto in kprotos.items()])\n",
    "    plt.plot(kprotos.keys(), [kproto.inertia_ for _, kproto in kprotos.items()])\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Sum of Squared Errors')\n",
    "\n",
    "kprotos = run_kmeans(normalized_speakers, start=2, end=6)\n",
    "plot_sse(kprotos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sse.png\" style=\"width: 500px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Helper function for printing various graph properties\n",
    "def describe_graph(G):\n",
    "    \"\"\"\n",
    "    Reference: tutorial Handling networks.\n",
    "    \"\"\"\n",
    "    print(nx.info(G))\n",
    "    if nx.is_connected(G):\n",
    "        print(\"Avg. Shortest Path Length: %.4f\" %nx.average_shortest_path_length(G))\n",
    "        print(\"Diameter: %.4f\" %nx.diameter(G)) # Longest shortest path\n",
    "    else:\n",
    "        print(\"Graph is not connected\")\n",
    "        print(\"Diameter and Avg shortest path length are not defined!\")\n",
    "    print(\"Sparsity: %.4f\" %nx.density(G))  # #edges/#edges-complete-graph\n",
    "    # #closed-triplets(3*#triangles)/#all-triplets\n",
    "    print(\"Global clustering coefficient aka Transitivity: %.4f\" %nx.transitivity(G))\n",
    "\n",
    "def draw_graph(graph):\n",
    "    nx.draw_spring(graph, with_labels=False, alpha=0.8)\n",
    "\n",
    "def degree_histogram_directed(G, in_degree=False, out_degree=False):\n",
    "    \"\"\"\n",
    "    Reference: exam 2020\n",
    "    \"\"\"\n",
    "    nodes = G.nodes()\n",
    "    if in_degree:\n",
    "        in_degree = dict(G.in_degree())\n",
    "        degseq=[in_degree.get(k,0) for k in nodes]\n",
    "    elif out_degree:\n",
    "        out_degree = dict(G.out_degree())\n",
    "        degseq=[out_degree.get(k,0) for k in nodes]\n",
    "    else:\n",
    "        degseq=[v for k, v in G.degree()]\n",
    "    dmax=max(degseq)+1\n",
    "    freq= [ 0 for d in range(dmax) ]\n",
    "    for d in degseq:\n",
    "        freq[d] += 1\n",
    "    return freq\n",
    "# Usage: \n",
    "# in_degree = degree_histogram_directed(wikiG, in_degree=True)\n",
    "# out_degree = degree_histogram_directed(wikiG, out_degree=True)\n",
    "# plt.loglog(range(len(in_degree)), in_degree)\n",
    "# plt.loglog(range(len(out_degree)), out_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weakly/strongly connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Is the Wikipedia link network strongly connected? If not, print the number of strongly connected components. \n",
    "print(f'Graph wikiG is strongly connected: {nx.is_strongly_connected(wikiG)}')\n",
    "print(f'Number of weakly connected components: {nx.number_strongly_connected_components(wikiG)}')\n",
    "# Additionally, print the number of nodes and edges of the subgraph corresponding to the largest strongly connected component.\n",
    "largest_cc = max(nx.strongly_connected_components(wikiG), key=len)\n",
    "print(f'Number of nodes of largest connected component: {len(largest_cc)}')\n",
    "H = wikiG.subgraph(list(largest_cc))\n",
    "print(f'Nodes: {len(H.nodes())}, edges: {len(H.edges())}, edges/nodes: {len(H.edges())/len(H.nodes()):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66843f79b43cc12be887547e03254aa6f4ef8e90ed8571877185a11b782708e1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('adaexam': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
